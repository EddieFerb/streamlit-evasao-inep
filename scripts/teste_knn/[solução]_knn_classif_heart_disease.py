# -*- coding: utf-8 -*-
"""[solução] knn-classif-heart-disease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aZnmn8Su-XxS7q1ABVaXq_o7SsKedZNa
"""

# Exercício 2: Predição de Doença Cardíaca com KNN
# ------------------------------------------------
# Objetivo:
# Aplicar o algoritmo KNN para prever a presença de doença cardíaca em pacientes.
# Inclui pré-processamento, escolha de K e avaliação com cross-validation e hold-out.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# ==========================
# Carregamento do dataset
# ==========================
# Dataset disponível no repositório UCI:
# https://archive.ics.uci.edu/ml/datasets/heart+disease
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

# Definição das colunas
columns = [
    "age", "sex", "cp", "trestbps", "chol", "fbs",
    "restecg", "thalach", "exang", "oldpeak", "slope",
    "ca", "thal", "target"
]

df = pd.read_csv(url, names=columns)

print("Dimensões do dataset:", df.shape)
print(df.head())

# ==========================
# Pré-processamento
# ==========================
# Substituir valores "?" por NaN e remover linhas com valores ausentes
df = df.replace("?", np.nan)
df = df.dropna()

# Conversão para float
df = df.astype(float)

# O target possui valores de 0 a 4, onde 0 = sem doença e 1-4 = com doença
df["target"] = df["target"].apply(lambda x: 1 if x > 0 else 0)

# Distribuição das classes
print(df["target"].value_counts())
print("\nProporção das classes (%):")
print(df["target"].value_counts(normalize=True) * 100)

# Separação em variáveis independentes (X) e dependente (y)
X = df.drop("target", axis=1)
y = df["target"]

# Padronização dos dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ==========================
# Escolha de K (pela maior acurácia)
# ==========================
k_range = range(1, 21)
mean_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_scaled, y, cv=10, scoring="accuracy")
    mean_scores.append(scores.mean())

plt.plot(k_range, mean_scores, marker="o")
plt.xlabel("Número de vizinhos (K)")
plt.ylabel("Acurácia média (cross-validation)")
plt.title("Acurácia média para diferentes valores de K")
plt.show()

# Melhor K é aquele que obteve maior acurácia média
best_k = k_range[np.argmax(mean_scores)]
print("Melhor valor de K encontrado:", best_k)

# ==========================
# Hold-out
# ==========================
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.20, random_state=100, stratify=y)

knn_holdout = KNeighborsClassifier(n_neighbors=best_k)
knn_holdout.fit(X_train, y_train)
y_pred = knn_holdout.predict(X_test)

print("\n[Hold-out] Acurácia:", accuracy_score(y_test, y_pred))
print("\n[Hold-out] Relatório de Classificação:\n", classification_report(y_test, y_pred))
print("\n[Hold-out] Matriz de Confusão:\n", confusion_matrix(y_test, y_pred))

# ==========================
# Cross-Validation
# ==========================
knn_cv = KNeighborsClassifier(n_neighbors=best_k)
cv_scores = cross_val_score(knn_cv, X_scaled, y, cv=10, scoring="accuracy")

print("\n[Cross-Validation] Acurácias individuais:", cv_scores)
print("[Cross-Validation] Média da acurácia:", cv_scores.mean())
print("[Cross-Validation] Desvio padrão:", cv_scores.std())